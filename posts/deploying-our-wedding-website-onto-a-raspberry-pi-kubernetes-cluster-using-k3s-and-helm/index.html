<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Deploying our Wedding Website onto a Raspberry Pi Kubernetes Cluster using K3s and Helm - Edd Mann</title>
<meta name=description content="Learn how to deploy a PHP web application on a Raspberry Pi Kubernetes cluster using K3s and Helm, covering setup, containerisation, Helm charts, and public access via SSL."><meta name=author content="Edd Mann"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZPQ7WHNXH4"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZPQ7WHNXH4")}</script><meta itemprop=name content="Deploying our Wedding Website onto a Raspberry Pi Kubernetes Cluster using K3s and Helm"><meta itemprop=description content="Over the past couple of weeks, I have been considering how feasible it would be to deploy the wedding website I released earlier this year on Kubernetes using a Raspberry Pi cluster. After a little research, this past bank holiday weekend, I set myself the goal of building and deploying the web application on Kubernetes by the end of the Monday bank holiday! In this post, I would like to discuss how I went about achieving this goal and what I learnt in the process."><meta itemprop=datePublished content="2022-04-29T00:00:00+00:00"><meta itemprop=dateModified content="2022-04-29T00:00:00+00:00"><meta itemprop=wordCount content="2488"><meta itemprop=keywords content="Kubernetes,Raspberry-Pi,Helm,Php"><meta property="og:url" content="https://eddmann.com/posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/"><meta property="og:site_name" content="Edd Mann"><meta property="og:title" content="Deploying our Wedding Website onto a Raspberry Pi Kubernetes Cluster using K3s and Helm"><meta property="og:description" content="Over the past couple of weeks, I have been considering how feasible it would be to deploy the wedding website I released earlier this year on Kubernetes using a Raspberry Pi cluster. After a little research, this past bank holiday weekend, I set myself the goal of building and deploying the web application on Kubernetes by the end of the Monday bank holiday! In this post, I would like to discuss how I went about achieving this goal and what I learnt in the process."><meta property="og:locale" content="en_GB"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-29T00:00:00+00:00"><meta property="article:modified_time" content="2022-04-29T00:00:00+00:00"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Raspberry-Pi"><meta property="article:tag" content="Helm"><meta property="article:tag" content="Php"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deploying our Wedding Website onto a Raspberry Pi Kubernetes Cluster using K3s and Helm"><meta name=twitter:description content="Over the past couple of weeks, I have been considering how feasible it would be to deploy the wedding website I released earlier this year on Kubernetes using a Raspberry Pi cluster. After a little research, this past bank holiday weekend, I set myself the goal of building and deploying the web application on Kubernetes by the end of the Monday bank holiday! In this post, I would like to discuss how I went about achieving this goal and what I learnt in the process."><meta name=twitter:site content="@edd_mann"><link rel=stylesheet href=/css/style.min.c3ee6cab58dd3fee7e6960b0391e996ba9254804702bc60a52ec5e0e3591a018.css integrity="sha256-w+5sq1jdP+5+aWCwOR6Za6klSARwK8YKUuxeDjWRoBg="><link rel=preload as=image href=/assets/x.svg><link rel=preload as=image href=/assets/github.svg><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://eddmann.com/posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/><script>document.documentElement.setAttribute("data-theme",localStorage.getItem("theme")||window.matchMedia("(prefers-color-scheme: dark)").matches&&"dark"||"light")</script><script src=/script.min.164d56ac6f4c8613b0b7109fd1b4d9dfdce1d004edab8e1afb7240013f9221d7.js integrity="sha256-Fk1WrG9MhhOwtxCf0bTZ39zh0ATtq44a+3JAAT+SIdc=" defer></script></head><body><header class="site-header l-wrapper"><a href=https://eddmann.com/><h3 class=site-header__title>Edd Mann
<span style=--url:url(/assets/code.svg)>Developer</span></h3></a><button class=site-header__mobile-navigation-button type=button title="Toggle mobile site navigation" aria-label="Toggle mobile site navigation" aria-expanded=false aria-controls=site-navigation></button><div class=site-header__navigation id=site-navigation><nav aria-label="Primary navigation"><ul class=site-header__primary-navigation><li><a href=/archive>Archive</a></li><li><a href=/projects>Projects</a></li><li><a href=/about>About</a></li></ul></nav><nav aria-label="Social links"><ul class=site-header__social-navigation><li><a class=social-icon style=--url:url(/assets/x.svg) href=https://x.com/edd_mann rel="external noopener" target=_blank>Twitter (X)</a></li><li><a class=social-icon style=--url:url(/assets/github.svg) href=https://github.com/eddmann rel="external noopener" target=_blank>GitHub</a></li></ul></nav></div></header><main class=l-wrapper><article><header class=l-page-title><h1 class=u-transition-between-pages style=--id:deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm>Deploying our Wedding Website onto a Raspberry Pi Kubernetes Cluster using K3s and Helm</h1><time datetime=2022-04-29T00:00:00Z class=published-at>Apr 29, 2022</time></header><main class=u-prose><p>Over the past couple of weeks, I have been considering how feasible it would be to deploy the <a href=https://github.com/eddmann/our-wedding-website rel="external noopener" target=_blank>wedding website</a> I released earlier this year on Kubernetes using a Raspberry Pi cluster.
After a little research, this past bank holiday weekend, I set myself the goal of building and deploying the web application on Kubernetes by the end of the Monday bank holiday!
In this post, I would like to discuss how I went about achieving this goal and what I learnt in the process.</p><h2 id=constructing-the-cluster>Constructing the Cluster</h2><p>Thanks to a previous project, I already had several Raspberry Pis gathering dust on the shelf, so I did not have to invest in any additional hardware (which was fortunate due to the <a href=https://www.raspberrypi.com/news/production-and-supply-chain-update/ rel="external noopener" target=_blank>shortages at this time</a>).
As such, for the cluster build, I opted to use 1x <a href=https://www.raspberrypi.com/products/raspberry-pi-4-model-b/ rel="external noopener" target=_blank>Raspberry Pi 4 Model B (4GB RAM)</a> as my server (master) node and 2x <a href=https://www.raspberrypi.com/products/raspberry-pi-3-model-b-plus/ rel="external noopener" target=_blank>Raspberry Pi 3 Model B+</a> as agent (worker) nodes.
Having seen several Raspberry Pi <a href=https://magpi.raspberrypi.com/articles/build-a-raspberry-pi-cluster-computer rel="external noopener" target=_blank>cluster</a> <a href=https://www.raspberrypi.com/tutorials/cluster-raspberry-pi-tutorial/ rel="external noopener" target=_blank>builds</a> <a href=https://anthonynsimon.com/blog/kubernetes-cluster-raspberry-pi/ rel="external noopener" target=_blank>online</a>, mine is by no means the most impressive, but it does provide sufficient distributed compute to explore many features Kubernetes has to offer.</p><p><picture><source type=image/webp srcset="/posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/cluster_hu_e24ed1f80e12937.webp 350w, /posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/cluster_hu_56dd8a7f2ca85bd5.webp 700w, /posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/cluster_hu_20049eef84db7066.webp 1280w"><source type=image/jpeg srcset="/posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/cluster_hu_e13edfe806958618.jpg 350w, /posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/cluster_hu_9bb765e86240b328.jpg 700w, /posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/cluster_hu_e3890751e91a8259.jpg 1280w"><img src=/posts/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/cluster_hu_9bb765e86240b328.jpg alt="My Raspberry Pi Kubernetes cluster" loading=lazy></picture></p><p>In a previous revision of the website, I had explored using a <a href=https://github.com/eddmann/aws-k8s-cluster-terraform rel="external noopener" target=_blank>single-master node Kubernetes setup</a>, which required me to bootstrap and configure many aspects of the cluster.
Fortunately, since this work, <a href=https://k3s.io/ rel="external noopener" target=_blank>K3s</a> has joined the party, providing a Kubernetes offering with a small resource footprint, ideal for low-powered devices such as the Raspberry Pi.
For a project such as mine, this was an ideal use case, allowing me to concentrate efforts on the application deployment itself.</p><h2 id=configuring-the-nodes>Configuring the Nodes</h2><p>To flash the desired OS to the Raspberry Pi micro-SD cards, I opted to use the <a href=https://www.raspberrypi.com/software/ rel="external noopener" target=_blank>Raspberry Pi Imager</a>.
On top of this, I also found a tool called <a href=https://github.com/alexellis/k3sup rel="external noopener" target=_blank>k3sup</a>, which is a small binary executable you can download on your host machine to assist in configuring each K3s node via SSH.</p><p>Each Raspberry Pi associated with the cluster was configured using the following procedure.
<em>Note:</em> These instructions are correct as of the time of writing.</p><ol><li>Open the Raspberry Pi Imager application and select <em>Raspberry Pi OS Lite (64-bit, Debian Bullseye)</em>.
Update the desired hostname/SSH credentials, and then flash the image to the micro-SD card.</li><li>Boot up the Pi.</li><li>Copy your host SSH key to the Pi by running: <code>ssh-copy-id pi@HOSTNAME.local</code></li><li><em>[On the Pi]</em> Update the Pi&rsquo;s static IP address defined within <code>/etc/dhcpcd.conf</code> by appending the following configuration:<pre tabindex=0><code>interface eth0
static ip_address={DESIRED_PI_IP}/24
static routers={ROUTER_IP}
static domain_name_servers={DNS_IP}
</code></pre></li><li><em>[On the Pi]</em> To enable the required kernel container functionality, append the following to the <code>/boot/cmdline.txt</code> file line:
<code>cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code></li><li><em>[On the Pi]</em> Reboot the Pi.</li><li>Set up the <em>server</em> node using the k3sup tool by executing:
<code>k3sup install --ip {SERVER_IP} --user pi</code>
Upon successful installation, you will be presented with a <code>kubeconfig</code> file on your host machine, which can be used to authenticate and interact with your newly formed cluster.</li><li>Once the <em>server</em> has been configured, you can follow the above procedure again to flash and configure the desired <em>agent</em> nodes.
However, instead of installing the <em>server</em> using the k3sup tool, you will <strong>join</strong> the existing cluster by executing:
<code>k3sup join --ip {AGENT_IP} --server-ip {SERVER_IP} --user pi</code></li></ol><p>Once all the Raspberry Pi&rsquo;s had joined the cluster I was ready to begin work on making the required modifications to the application.
Using <a href=https://kubernetes.io/docs/reference/kubectl/ rel="external noopener" target=_blank>kubectl</a> I could now access the built cluster like so.</p><pre tabindex=0><code>&gt; kubectl get nodes -o wide

NAME           STATUS   ROLES                  AGE    VERSION        INTERNAL-IP    OS-IMAGE
kube-server    Ready    control-plane,master   3d5h   v1.22.7+k3s1   192.168.1.40   Debian GNU/Linux 11 (bullseye)
kube-agent-1   Ready    &lt;none&gt;                 3d5h   v1.22.7+k3s1   192.168.1.41   Debian GNU/Linux 11 (bullseye)
kube-agent-2   Ready    &lt;none&gt;                 3d5h   v1.22.7+k3s1   192.168.1.42   Debian GNU/Linux 11 (bullseye)
</code></pre><h2 id=modifying-the-web-application>Modifying the Web Application</h2><p>The original web application had been developed to be run within an FaaS environment, more specifically <a href=https://github.com/eddmann/our-wedding-website/blob/main/app/serverless.yml rel="external noopener" target=_blank>AWS Lambda</a>.
I wanted to refrain from having to make any unnecessary changes to the underlying application to speed up project progress.
My initial task was to develop the containerised environment within which the PHP application could run.</p><p>I opted to use an <a href=https://github.com/eddmann/our-wedding-website/blob/main/kube/docker/Dockerfile rel="external noopener" target=_blank>Alpine-based image</a>, which took advantage of the great <a href=https://github.com/mlocati/docker-php-extension-installer rel="external noopener" target=_blank>docker-php-extension-installer</a> to include the desired PHP extensions.
I also decided against splitting up the HTTP proxy and PHP-FPM responsibilities for this project and instead baked Caddy into the single image.
To handle the different responsibilities required of the application, I included a custom <a href=https://github.com/eddmann/our-wedding-website/blob/main/kube/docker/docker-entrypoint.sh rel="external noopener" target=_blank>Docker entrypoint script</a>, which allowed the single built image to either handle web traffic, become a long-polling asynchronous worker, or apply outstanding database migrations.
This is by no means best practice, but it was a trade-off to reach the desired project outcome within the deadline.</p><p>Using a multi-stage Docker build allowed me to pull in a Node image to handle compiling the application assets and include them in the final deliverable.
This image was not intended to be used in a development setting, so other tooling present within the Lambda-focused environment was intentionally omitted.</p><p>With the target platform being a Raspberry Pi, I had to ensure that the image being built supported the ARM64 architecture.
To achieve this, I used <a href=https://docs.docker.com/buildx/working-with-buildx/ rel="external noopener" target=_blank>Docker Buildx</a>, which allowed me to build (although rather slowly on my MacBook Pro) two platform-specific images.
These were then pushed to my container registry of choice, GitHub Container Registry (GHCR).</p><pre tabindex=0><code>docker buildx build \
  --platform linux/amd64,linux/arm64 \
  --tag ghcr.io/eddmann/our-wedding-website-kube:latest \
  --file docker/Dockerfile \
  --target prod \
  --push .
</code></pre><p>This allowed me to experiment with the image on my x86 64-bit host machine, as well as on the Raspberry Pi itself.
With the images now built and shipped to GHCR, I then developed a <a href=https://github.com/eddmann/our-wedding-website/blob/main/kube/docker/docker-compose.yml rel="external noopener" target=_blank>Docker Compose environment</a> which could be run locally to replicate the desired Kubernetes deployment.
This environment included the single built image being used in the different desired roles: as a web application, a worker, and a one-off database migration execution.</p><p>Within the AWS Lambda-based environment, I had used DynamoDB to store the web application sessions and SQS to handle the worker queue.
For this deployment, I opted instead to use Redis for the web application session management and RabbitMQ to handle the asynchronous message bus.
Fortunately, I was able to keep the same local setup used for PostgreSQL, which was used for persistent data storage.</p><p>Thanks to following the practices outlined in the <a href=https://12factor.net/ rel="external noopener" target=_blank>twelve-factor app</a>, there was only a single additional Symfony <a href=https://github.com/eddmann/our-wedding-website/blob/main/kube/docker/redis.yaml rel="external noopener" target=_blank>configuration file</a> that was required to be added to the final built image.
All other configuration changes were achievable via outside control, using exposed environment variables.</p><p>With the application now configured and running locally using Docker Compose, it was time to implement this environment within Kubernetes using a <a href=https://github.com/eddmann/our-wedding-website/tree/main/kube/helm rel="external noopener" target=_blank>Helm chart</a>.</p><h2 id=building-and-deploying-the-helm-chart>Building and Deploying the Helm Chart</h2><p>I was very fortunate to be able to borrow a lot of the resource definitions I had written for the previous web application&rsquo;s <a href=https://github.com/eddmann/our-wedding-website/tree/main/kube/helm rel="external noopener" target=_blank>Helm chart</a>.
To keep this application isolated from other services on the cluster, I created and used a specific namespace.
All secrets were stored as Kubernetes secret resources and injected in as environment variables to the relevant pod containers.
As the application image was stored privately within GHCR, I was required to provide the GitHub personal access token capable of pulling that image.
I opted to let <em>kubectl</em> create the underlying resource, which was preferable as this process has several steps.</p><pre tabindex=0><code>kubectl create secret docker-registry github \
  --namespace our-wedding-website \
  --docker-server=https://ghcr.io \
  --docker-username=irrelevant-user \
  --docker-password=#PAT#
</code></pre><p>As this was an experimental project, I opted to keep <a href=https://github.com/eddmann/our-wedding-website/blob/main/kube/helm/templates/postgres.yaml rel="external noopener" target=_blank>PostgreSQL</a> confined to a single replica stateful set.
This allowed me to persist the data across node restarts but did not cater for node failure and database failover.
I also opted to run single replica instances of the <a href=https://github.com/eddmann/our-wedding-website/blob/main/kube/helm/templates/rabbitmq.yaml rel="external noopener" target=_blank>RabbitMQ</a> and <a href=https://github.com/eddmann/our-wedding-website/blob/main/kube/helm/templates/redis.yaml rel="external noopener" target=_blank>Redis</a> headless services.
Again, this decision was made out of ease as opposed to best practice.</p><p>The database migration was mapped to a <a href=https://github.com/eddmann/our-wedding-website/blob/main/kube/helm/templates/migrate-db.yaml rel="external noopener" target=_blank>Kubernetes job</a>, which included the Helm revision number in its name, resulting in a new job being triggered upon each Helm deployment.
During my research on handling database migrations, I came <a href=https://andrewlock.net/deploying-asp-net-core-applications-to-kubernetes-part-8-running-database-migrations-using-jobs-and-init-containers/ rel="external noopener" target=_blank>across a method</a> to ensure that the pending web/worker services would not be added to the service rotation until the dependent migration job had succeeded.
This was achieved by way of an Init Container attached to the web/worker deployment definitions, which waited for the desired job to be completed before starting the actual pod containers.
Unfortunately, the Docker image in question did not support the ARM architecture, and as such, I opted to omit this addition in the interest of time.</p><p>Finally, the web application service was exposed via an Ingress resource using the pre-included Traefik HTTP proxy.
I was now able to deploy the Helm chart to my Raspberry Pi cluster.</p><pre tabindex=0><code>helm upgrade our-wedding-website ./helm \
  --install \
  --namespace our-wedding-website \
  --values ./helm/values.yaml \
  --values ./helm/secrets.yaml
</code></pre><p>In doing so, I was able to access the web application by proxying into the service (via <code>kubectl port-forward</code>) or setting one of the Raspberry Pi local network IPs as the resolved hostname destination within my <code>/etc/hosts</code> 🎉.</p><pre tabindex=0><code>&gt; kubectl get pods -n our-wedding-website

NAME                        READY   STATUS      RESTARTS      AGE
rabbitmq-7f69f76c99-p7m2j   1/1     Running     1 (18m ago)   1h
postgres-0                  1/1     Running     1 (18m ago)   1h
redis-657cfb4cbf-nffct      1/1     Running     1 (18m ago)   1h
web-7d589fcc5b-6zdcq        1/1     Running     1 (18m ago)   1h
migrate-db-11--1-v5gdh      0/1     Completed   0             1h
worker-65f8548ff-vpklh      1/1     Running     2 (10m ago)   1h
</code></pre><p>I could also now experiment with scaling out the web and worker services via <em>kubectl</em>.</p><pre tabindex=0><code>&gt; kubectl scale --replicas=3 deployment/web
&gt; kubectl scale --replicas=3 deployment/worker
&gt; kubectl get pods -n our-wedding-website -o wide

NAME                        READY   STATUS      RESTARTS       AGE    IP            NODE
rabbitmq-7f69f76c99-p7m2j   1/1     Running     1 (18m ago)    1h     10.42.1.46    kube-agent-1
postgres-0                  1/1     Running     1 (18m ago)    1h     10.42.0.132   kube-server
redis-657cfb4cbf-nffct      1/1     Running     1 (18m ago)    1h     10.42.0.134   kube-server
web-7d589fcc5b-6zdcq        1/1     Running     0              1h     10.42.0.127   kube-server
migrate-db-11--1-v5gdh      0/1     Completed   0              1h     10.42.0.139   kube-server
worker-65f8548ff-vpklh      1/1     Running     15 (53m ago)   1h     10.42.0.128   kube-server
worker-65f8548ff-2mwcz      1/1     Running     0              14s    10.42.2.56    kube-agent-2
worker-65f8548ff-tm4sf      1/1     Running     0              14s    10.42.2.57    kube-agent-2
web-7d589fcc5b-qwzv8        1/1     Running     0              14s    10.42.0.140   kube-server
web-7d589fcc5b-m2fhx        1/1     Running     0              14s    10.42.2.55    kube-agent-2
</code></pre><h2 id=accessing-the-website-from-the-internet>Accessing the Website from the Internet</h2><p>Being able to access and interact with the website on my local network was pretty cool.
However, the end goal of this long weekend was to be able to access the web application on the public internet, in a similar fashion to the AWS Lambda deployment.
The easiest means of achieving this would be to expose the port on my home router and port-forward traffic to one of my Raspberry Pi cluster nodes.
Sadly, I do not have a static IP address provided by my ISP and also do not like the sound of exposing ports to the public internet.
As such, I had to find another means of exposing the service to the public internet.</p><p>After a little research, I was pleasantly surprised to find a project called <a href=https://inlets.dev/ rel="external noopener" target=_blank>Inlets</a>, which did exactly what I was looking for!
Better still, it included a <a href=https://github.com/inlets/inlets-operator rel="external noopener" target=_blank>Kubernetes operator</a> that handled provisioning the required Kubernetes resources.
The project itself creates a VPS instance (with a static IP) on your desired cloud provider (i.e. EC2 in the case of AWS) and uses this as an exit node for traffic that is securely tunnelled between your local Kubernetes cluster and requests coming in through the public VPS.
This enables you to access the Ingress service (ports 80 and 443) via the VPS IP address, without the need to expose any local router ports.
The catch, however, was that without a paid-for plan, you were only capable of creating insecure HTTP (port 80) connections 😢.
I really wanted to replicate the same HTTPS experience that the AWS Lambda environment had.
I did look at the pricing options, but as this was a small experimental project that was going to be torn down in the near future, I did not feel I could justify the cost.</p><h3 id=creating-my-own-tunnel>Creating My Own Tunnel</h3><p>It was back to the drawing board&mldr; but this time, the idea of setting up some form of tunnel between my local Kubernetes cluster and a remote VPS stuck in my head.
With this idea, I experimented with setting up a <a href=https://youtu.be/Wp7boqm3Xts rel="external noopener" target=_blank>reverse SSH tunnel</a> between one of the nodes&rsquo; Ingress-exposed ports (80 and 443) and an EC2 instance.
Due to these ports being so low, I was unfortunately required to authenticate with the EC2 instance (exit node) as the root user 😬.
Having configured the following SSH daemon configuration options, <code>GatewayPorts yes</code> and <code>PermitRootLogin yes</code>, I was able to initiate the reverse tunnel in the background as follows.</p><pre tabindex=0><code>ssh -R80:127.0.0.1:80 -R443:127.0.0.1:443 -N root@EXIT_NODE_IP -i {KEY} &amp;
</code></pre><p>With this running in the background, I was now able to update the desired domain&rsquo;s DNS record to point to the EC2 instance&rsquo;s public IP address and access the web application via both HTTP and HTTPS.
There is, of course, a huge caveat to using this approach, as it again introduced another single point of failure.
By design within Kubernetes, you can access the exposed Ingress service using any of the provided node IPs.
With this approach, we are limiting entry to just one.
Again, as this was just an experimental project, this approach would suffice.</p><p>When visiting the web application via HTTPS, however, the response returned a self-signed certificate that Traefik had created.
We were now capable of changing that!</p><h3 id=issuing-ssl-certificates-using-cert-manager>Issuing SSL Certificates using cert-manager</h3><p>With public access now available, I could configure <a href=https://cert-manager.io/ rel="external noopener" target=_blank>cert-manager</a> within my Kubernetes cluster to handle issuing SSL certificates via Let&rsquo;s Encrypt.
This tool not only assists in issuing the SSL certificates, but it also provides a means of automatically renewing certificates that are close to expiry.
Using the <a href=https://cert-manager.io/docs/installation/ rel="external noopener" target=_blank>installation instructions</a> present on their website, I was able to set up <em>cert-manager</em> and the associated staging/production Let&rsquo;s Encrypt <em>ClusterIssuer</em> resources.
I was then able to add the required annotations to the Ingress resource, instructing <em>cert-manager</em> to issue and use a Let&rsquo;s Encrypt certificate.
I initially used the staging Let&rsquo;s Encrypt issuer to ensure that my configuration was correct before moving on to generating the production-ready certificate.
With the certificate in place, I could now access the web application on the public internet via HTTPS.</p><h2 id=conclusion>Conclusion</h2><p>By the end of the long weekend, the web application was finally live! 🎉</p><p>I am very happy with the resulting Kubernetes-based deployment.
Getting the chance to construct and configure the Kubernetes cluster itself was a lot of fun and was made easier thanks to K3s and the k3sup tool.
Being able to keep so much of the original application intact and only apply additive changes was a great experience.
It highlighted the importance of the twelve-factor app methodology and the use of environment variables.
Getting hands-on experience using Kubernetes resources and tools such as Helm and cert-manager was invaluable.
Finally, being able to produce a deliverable on par with what is currently running in the AWS Lambda environment was very rewarding.</p><p>I hope this has piqued your interest in trying to set up and configure your own Kubernetes cluster!</p></main><footer class=post-footer><ul class="tags tags--large"><li><a href=/archive/tag/kubernetes>kubernetes</a></li><li><a href=/archive/tag/raspberry-pi>raspberry-pi</a></li><li><a href=/archive/tag/helm>helm</a></li><li><a href=/archive/tag/php>php</a></li></ul><div class=related><h3 class=related__title>Related Posts</h3><ul class=related__posts><li><a href=/posts/separating-out-the-lambda-bref-runtime-from-your-projects-composer-dependencies/>Separating out the Lambda Bref runtime from your project's Composer dependencies</a></li><li><a href=/posts/our-wedding-website-three-years-in-the-making/>Our Wedding Website, Three Years in the Making...</a></li><li><a href=/posts/creating-a-time-lapse-camera-with-hypriotos-docker-using-a-raspberry-pi-3b-plus-and-raspberry-pi-zero-w/>Creating a Time-lapse Camera with HypriotOS/Docker using a Raspberry Pi 3B+ and Raspberry Pi Zero W</a></li><li><a href=/posts/managing-newlines-and-unicode-within-javascript-and-php/>Managing Newlines and Unicode within JavaScript and PHP</a></li><li><a href=/posts/designing-immutable-concepts-with-transient-mutation-in-php/>Designing Immutable Concepts with Transient Mutation in PHP</a></li></ul></div></footer></article><div class=scroll-watcher></div></main><footer class="site-footer l-wrapper"><div>&copy; 2025, Edd Mann</div><button class=site-footer__theme-toggle type=button title="Toggle theme" aria-label="Toggle theme"><svg fill="currentcolor" viewBox="0 0 32 32" role="img"><title>Theme toggle icon</title><desc>A circle representing the moon for toggling theme</desc><path d="M16 .5C7.4.5.5 7.4.5 16S7.4 31.5 16 31.5 31.5 24.6 31.5 16 24.6.5 16 .5zm0 28.1V3.4C23 3.4 28.6 9 28.6 16S23 28.6 16 28.6z"/></svg></button></footer></body></html>