<!doctype html>
<html lang="en">
    <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Learn how to deploy a PHP web application on a Raspberry Pi Kubernetes cluster using K3s and Helm, covering setup, containerisation, Helm charts, and public access via SSL.">

    <title>
        
            Deploying our Wedding Website onto a Raspberry Pi Kubernetes Cluster using K3s and Helm &middot; Edd Mann
        
    </title>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZPQ7WHNXH4"></script>
    <script>
       window.dataLayer = window.dataLayer || [];
       function gtag(){dataLayer.push(arguments);}
       gtag('js', new Date());
       gtag('config', 'G-ZPQ7WHNXH4');
    </script>

    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/styles.css">

    <link rel="shortcut icon" href="/favicon.ico">

    <link rel="alternate" href="/rss.xml" title="" type="application/rss+xml">

    
</head>

    <body>
        <div class="container content">
            <header class="masthead">
                <h3 class="masthead-title">
                    <a href="/" title="Home">Edd Mann</a>
                    <small><i class="fa fa-code"></i> Developer</small>
                </h3>
                <nav>
                    <a href="https://twitter.com/edd_mann"><i class="fa fa-twitter"></i></a>
                    <a href="https://github.com/eddmann"><i class="fa fa-github"></i></a>
                    <a href="mailto:the@eddmann.com"><i class="fa fa-envelope"></i></a>
                </nav>
                <div class="cf"></div>
            </header>

            <main>
                <article class="post">
    <h1 class="post-title">Deploying our Wedding Website onto a Raspberry Pi Kubernetes Cluster using K3s and Helm</h1>
    <time datetime="2022-04-29T00:00:00+00:00" class="post-date">29 Apr 2022</time>
    <p>Over the past couple of weeks, I have been considering how feasible it would be to deploy the <a href="https://github.com/eddmann/our-wedding-website">wedding website</a> I released earlier this year on Kubernetes using a Raspberry Pi cluster.
After a little research, this past bank holiday weekend, I set myself the goal of building and deploying the web application on Kubernetes by the end of the Monday bank holiday!
In this post, I would like to discuss how I went about achieving this goal and what I learnt in the process.</p>



<h2 id="constructing-the-cluster">Constructing the Cluster</h2>

<p>Thanks to a previous project, I already had several Raspberry Pis gathering dust on the shelf, so I did not have to invest in any additional hardware (which was fortunate due to the <a href="https://www.raspberrypi.com/news/production-and-supply-chain-update/">shortages at this time</a>).
As such, for the cluster build, I opted to use 1x <a href="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/">Raspberry Pi 4 Model B (4GB RAM)</a> as my server (master) node and 2x <a href="https://www.raspberrypi.com/products/raspberry-pi-3-model-b-plus/">Raspberry Pi 3 Model B+</a> as agent (worker) nodes.
Having seen several Raspberry Pi <a href="https://magpi.raspberrypi.com/articles/build-a-raspberry-pi-cluster-computer">cluster</a> <a href="https://www.raspberrypi.com/tutorials/cluster-raspberry-pi-tutorial/">builds</a> <a href="https://anthonynsimon.com/blog/kubernetes-cluster-raspberry-pi/">online</a>, mine is by no means the most impressive, but it does provide sufficient distributed compute to explore many features Kubernetes has to offer.</p>

<p><img src="/uploads/deploying-our-wedding-website-onto-a-raspberry-pi-kubernetes-cluster-using-k3s-and-helm/cluster.jpg" alt="My Raspberry Pi Kubernetes cluster" /></p>

<p>In a previous revision of the website, I had explored using a <a href="https://github.com/eddmann/aws-k8s-cluster-terraform">single-master node Kubernetes setup</a>, which required me to bootstrap and configure many aspects of the cluster.
Fortunately, since this work, <a href="https://k3s.io/">K3s</a> has joined the party, providing a Kubernetes offering with a small resource footprint, ideal for low-powered devices such as the Raspberry Pi.
For a project such as mine, this was an ideal use case, allowing me to concentrate efforts on the application deployment itself.</p>

<h2 id="configuring-the-nodes">Configuring the Nodes</h2>

<p>To flash the desired OS to the Raspberry Pi micro-SD cards, I opted to use the <a href="https://www.raspberrypi.com/software/">Raspberry Pi Imager</a>.
On top of this, I also found a tool called <a href="https://github.com/alexellis/k3sup">k3sup</a>, which is a small binary executable you can download on your host machine to assist in configuring each K3s node via SSH.</p>

<p>Each Raspberry Pi associated with the cluster was configured using the following procedure.
<em>Note:</em> These instructions are correct as of the time of writing.</p>

<ol>
  <li>Open the Raspberry Pi Imager application and select <em>Raspberry Pi OS Lite (64-bit, Debian Bullseye)</em>.
Update the desired hostname/SSH credentials, and then flash the image to the micro-SD card.</li>
  <li>Boot up the Pi.</li>
  <li>Copy your host SSH key to the Pi by running: <code class="language-plaintext highlighter-rouge">ssh-copy-id pi@HOSTNAME.local</code></li>
  <li><em>[On the Pi]</em> Update the Piâ€™s static IP address defined within <code class="language-plaintext highlighter-rouge">/etc/dhcpcd.conf</code> by appending the following configuration:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interface eth0
static ip_address={DESIRED_PI_IP}/24
static routers={ROUTER_IP}
static domain_name_servers={DNS_IP}
</code></pre></div>    </div>
  </li>
  <li><em>[On the Pi]</em> To enable the required kernel container functionality, append the following to the <code class="language-plaintext highlighter-rouge">/boot/cmdline.txt</code> file line:
<code class="language-plaintext highlighter-rouge">cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code></li>
  <li><em>[On the Pi]</em> Reboot the Pi.</li>
  <li>Set up the <em>server</em> node using the k3sup tool by executing:
<code class="language-plaintext highlighter-rouge">k3sup install --ip {SERVER_IP} --user pi</code>
Upon successful installation, you will be presented with a <code class="language-plaintext highlighter-rouge">kubeconfig</code> file on your host machine, which can be used to authenticate and interact with your newly formed cluster.</li>
  <li>Once the <em>server</em> has been configured, you can follow the above procedure again to flash and configure the desired <em>agent</em> nodes.
However, instead of installing the <em>server</em> using the k3sup tool, you will <strong>join</strong> the existing cluster by executing:
<code class="language-plaintext highlighter-rouge">k3sup join --ip {AGENT_IP} --server-ip {SERVER_IP} --user pi</code></li>
</ol>

<p>Once all the Raspberry Piâ€™s had joined the cluster I was ready to begin work on making the required modifications to the application.
Using <a href="https://kubernetes.io/docs/reference/kubectl/">kubectl</a> I could now access the built cluster like so.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; kubectl get nodes -o wide

NAME           STATUS   ROLES                  AGE    VERSION        INTERNAL-IP    OS-IMAGE
kube-server    Ready    control-plane,master   3d5h   v1.22.7+k3s1   192.168.1.40   Debian GNU/Linux 11 (bullseye)
kube-agent-1   Ready    &lt;none&gt;                 3d5h   v1.22.7+k3s1   192.168.1.41   Debian GNU/Linux 11 (bullseye)
kube-agent-2   Ready    &lt;none&gt;                 3d5h   v1.22.7+k3s1   192.168.1.42   Debian GNU/Linux 11 (bullseye)
</code></pre></div></div>

<h2 id="modifying-the-web-application">Modifying the Web Application</h2>

<p>The original web application had been developed to be run within an FaaS environment, more specifically <a href="https://github.com/eddmann/our-wedding-website/blob/main/app/serverless.yml">AWS Lambda</a>.
I wanted to refrain from having to make any unnecessary changes to the underlying application to speed up project progress.
My initial task was to develop the containerised environment within which the PHP application could run.</p>

<p>I opted to use an <a href="https://github.com/eddmann/our-wedding-website/blob/main/kube/docker/Dockerfile">Alpine-based image</a>, which took advantage of the great <a href="https://github.com/mlocati/docker-php-extension-installer">docker-php-extension-installer</a> to include the desired PHP extensions.
I also decided against splitting up the HTTP proxy and PHP-FPM responsibilities for this project and instead baked Caddy into the single image.
To handle the different responsibilities required of the application, I included a custom <a href="https://github.com/eddmann/our-wedding-website/blob/main/kube/docker/docker-entrypoint.sh">Docker entrypoint script</a>, which allowed the single built image to either handle web traffic, become a long-polling asynchronous worker, or apply outstanding database migrations.
This is by no means best practice, but it was a trade-off to reach the desired project outcome within the deadline.</p>

<p>Using a multi-stage Docker build allowed me to pull in a Node image to handle compiling the application assets and include them in the final deliverable.
This image was not intended to be used in a development setting, so other tooling present within the Lambda-focused environment was intentionally omitted.</p>

<p>With the target platform being a Raspberry Pi, I had to ensure that the image being built supported the ARM64 architecture.
To achieve this, I used <a href="https://docs.docker.com/buildx/working-with-buildx/">Docker Buildx</a>, which allowed me to build (although rather slowly on my MacBook Pro) two platform-specific images.
These were then pushed to my container registry of choice, GitHub Container Registry (GHCR).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker buildx build \
  --platform linux/amd64,linux/arm64 \
  --tag ghcr.io/eddmann/our-wedding-website-kube:latest \
  --file docker/Dockerfile \
  --target prod \
  --push .
</code></pre></div></div>

<p>This allowed me to experiment with the image on my x86 64-bit host machine, as well as on the Raspberry Pi itself.
With the images now built and shipped to GHCR, I then developed a <a href="https://github.com/eddmann/our-wedding-website/blob/main/kube/docker/docker-compose.yml">Docker Compose environment</a> which could be run locally to replicate the desired Kubernetes deployment.
This environment included the single built image being used in the different desired roles: as a web application, a worker, and a one-off database migration execution.</p>

<p>Within the AWS Lambda-based environment, I had used DynamoDB to store the web application sessions and SQS to handle the worker queue.
For this deployment, I opted instead to use Redis for the web application session management and RabbitMQ to handle the asynchronous message bus.
Fortunately, I was able to keep the same local setup used for PostgreSQL, which was used for persistent data storage.</p>

<p>Thanks to following the practices outlined in the <a href="https://12factor.net/">twelve-factor app</a>, there was only a single additional Symfony <a href="https://github.com/eddmann/our-wedding-website/blob/main/kube/docker/redis.yaml">configuration file</a> that was required to be added to the final built image.
All other configuration changes were achievable via outside control, using exposed environment variables.</p>

<p>With the application now configured and running locally using Docker Compose, it was time to implement this environment within Kubernetes using a <a href="https://github.com/eddmann/our-wedding-website/tree/main/kube/helm">Helm chart</a>.</p>

<h2 id="building-and-deploying-the-helm-chart">Building and Deploying the Helm Chart</h2>

<p>I was very fortunate to be able to borrow a lot of the resource definitions I had written for the previous web applicationâ€™s <a href="https://github.com/eddmann/our-wedding-website/tree/main/kube/helm">Helm chart</a>.
To keep this application isolated from other services on the cluster, I created and used a specific namespace.
All secrets were stored as Kubernetes secret resources and injected in as environment variables to the relevant pod containers.
As the application image was stored privately within GHCR, I was required to provide the GitHub personal access token capable of pulling that image.
I opted to let <em>kubectl</em> create the underlying resource, which was preferable as this process has several steps.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create secret docker-registry github \
  --namespace our-wedding-website \
  --docker-server=https://ghcr.io \
  --docker-username=irrelevant-user \
  --docker-password=#PAT#
</code></pre></div></div>

<p>As this was an experimental project, I opted to keep <a href="https://github.com/eddmann/our-wedding-website/blob/main/kube/helm/templates/postgres.yaml">PostgreSQL</a> confined to a single replica stateful set.
This allowed me to persist the data across node restarts but did not cater for node failure and database failover.
I also opted to run single replica instances of the <a href="https://github.com/eddmann/our-wedding-website/blob/main/kube/helm/templates/rabbitmq.yaml">RabbitMQ</a> and <a href="https://github.com/eddmann/our-wedding-website/blob/main/kube/helm/templates/redis.yaml">Redis</a> headless services.
Again, this decision was made out of ease as opposed to best practice.</p>

<p>The database migration was mapped to a <a href="https://github.com/eddmann/our-wedding-website/blob/main/kube/helm/templates/migrate-db.yaml">Kubernetes job</a>, which included the Helm revision number in its name, resulting in a new job being triggered upon each Helm deployment.
During my research on handling database migrations, I came <a href="https://andrewlock.net/deploying-asp-net-core-applications-to-kubernetes-part-8-running-database-migrations-using-jobs-and-init-containers/">across a method</a> to ensure that the pending web/worker services would not be added to the service rotation until the dependent migration job had succeeded.
This was achieved by way of an Init Container attached to the web/worker deployment definitions, which waited for the desired job to be completed before starting the actual pod containers.
Unfortunately, the Docker image in question did not support the ARM architecture, and as such, I opted to omit this addition in the interest of time.</p>

<p>Finally, the web application service was exposed via an Ingress resource using the pre-included Traefik HTTP proxy.
I was now able to deploy the Helm chart to my Raspberry Pi cluster.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm upgrade our-wedding-website ./helm \
  --install \
  --namespace our-wedding-website \
  --values ./helm/values.yaml \
  --values ./helm/secrets.yaml
</code></pre></div></div>

<p>In doing so, I was able to access the web application by proxying into the service (via <code class="language-plaintext highlighter-rouge">kubectl port-forward</code>) or setting one of the Raspberry Pi local network IPs as the resolved hostname destination within my <code class="language-plaintext highlighter-rouge">/etc/hosts</code> ðŸŽ‰.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; kubectl get pods -n our-wedding-website

NAME                        READY   STATUS      RESTARTS      AGE
rabbitmq-7f69f76c99-p7m2j   1/1     Running     1 (18m ago)   1h
postgres-0                  1/1     Running     1 (18m ago)   1h
redis-657cfb4cbf-nffct      1/1     Running     1 (18m ago)   1h
web-7d589fcc5b-6zdcq        1/1     Running     1 (18m ago)   1h
migrate-db-11--1-v5gdh      0/1     Completed   0             1h
worker-65f8548ff-vpklh      1/1     Running     2 (10m ago)   1h
</code></pre></div></div>

<p>I could also now experiment with scaling out the web and worker services via <em>kubectl</em>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; kubectl scale --replicas=3 deployment/web
&gt; kubectl scale --replicas=3 deployment/worker
&gt; kubectl get pods -n our-wedding-website -o wide

NAME                        READY   STATUS      RESTARTS       AGE    IP            NODE
rabbitmq-7f69f76c99-p7m2j   1/1     Running     1 (18m ago)    1h     10.42.1.46    kube-agent-1
postgres-0                  1/1     Running     1 (18m ago)    1h     10.42.0.132   kube-server
redis-657cfb4cbf-nffct      1/1     Running     1 (18m ago)    1h     10.42.0.134   kube-server
web-7d589fcc5b-6zdcq        1/1     Running     0              1h     10.42.0.127   kube-server
migrate-db-11--1-v5gdh      0/1     Completed   0              1h     10.42.0.139   kube-server
worker-65f8548ff-vpklh      1/1     Running     15 (53m ago)   1h     10.42.0.128   kube-server
worker-65f8548ff-2mwcz      1/1     Running     0              14s    10.42.2.56    kube-agent-2
worker-65f8548ff-tm4sf      1/1     Running     0              14s    10.42.2.57    kube-agent-2
web-7d589fcc5b-qwzv8        1/1     Running     0              14s    10.42.0.140   kube-server
web-7d589fcc5b-m2fhx        1/1     Running     0              14s    10.42.2.55    kube-agent-2
</code></pre></div></div>

<h2 id="accessing-the-website-from-the-internet">Accessing the Website from the Internet</h2>

<p>Being able to access and interact with the website on my local network was pretty cool.
However, the end goal of this long weekend was to be able to access the web application on the public internet, in a similar fashion to the AWS Lambda deployment.
The easiest means of achieving this would be to expose the port on my home router and port-forward traffic to one of my Raspberry Pi cluster nodes.
Sadly, I do not have a static IP address provided by my ISP and also do not like the sound of exposing ports to the public internet.
As such, I had to find another means of exposing the service to the public internet.</p>

<p>After a little research, I was pleasantly surprised to find a project called <a href="https://inlets.dev/">Inlets</a>, which did exactly what I was looking for!
Better still, it included a <a href="https://github.com/inlets/inlets-operator">Kubernetes operator</a> that handled provisioning the required Kubernetes resources.
The project itself creates a VPS instance (with a static IP) on your desired cloud provider (i.e. EC2 in the case of AWS) and uses this as an exit node for traffic that is securely tunnelled between your local Kubernetes cluster and requests coming in through the public VPS.
This enables you to access the Ingress service (ports 80 and 443) via the VPS IP address, without the need to expose any local router ports.
The catch, however, was that without a paid-for plan, you were only capable of creating insecure HTTP (port 80) connections ðŸ˜¢.
I really wanted to replicate the same HTTPS experience that the AWS Lambda environment had.
I did look at the pricing options, but as this was a small experimental project that was going to be torn down in the near future, I did not feel I could justify the cost.</p>

<h2 id="creating-my-own-tunnel">Creating My Own Tunnel</h2>

<p>It was back to the drawing boardâ€¦ but this time, the idea of setting up some form of tunnel between my local Kubernetes cluster and a remote VPS stuck in my head.
With this idea, I experimented with setting up a <a href="https://youtu.be/Wp7boqm3Xts">reverse SSH tunnel</a> between one of the nodesâ€™ Ingress-exposed ports (80 and 443) and an EC2 instance.
Due to these ports being so low, I was unfortunately required to authenticate with the EC2 instance (exit node) as the root user ðŸ˜¬.
Having configured the following SSH daemon configuration options, <code class="language-plaintext highlighter-rouge">GatewayPorts yes</code> and <code class="language-plaintext highlighter-rouge">PermitRootLogin yes</code>, I was able to initiate the reverse tunnel in the background as follows.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh -R80:127.0.0.1:80 -R443:127.0.0.1:443 -N root@EXIT_NODE_IP -i {KEY} &amp;
</code></pre></div></div>

<p>With this running in the background, I was now able to update the desired domainâ€™s DNS record to point to the EC2 instanceâ€™s public IP address and access the web application via both HTTP and HTTPS.
There is, of course, a huge caveat to using this approach, as it again introduced another single point of failure.
By design within Kubernetes, you can access the exposed Ingress service using any of the provided node IPs.
With this approach, we are limiting entry to just one.
Again, as this was just an experimental project, this approach would suffice.</p>

<p>When visiting the web application via HTTPS, however, the response returned a self-signed certificate that Traefik had created.
We were now capable of changing that!</p>

<h2 id="issuing-ssl-certificates-using-cert-manager">Issuing SSL Certificates using cert-manager</h2>

<p>With public access now available, I could configure <a href="https://cert-manager.io/">cert-manager</a> within my Kubernetes cluster to handle issuing SSL certificates via Letâ€™s Encrypt.
This tool not only assists in issuing the SSL certificates, but it also provides a means of automatically renewing certificates that are close to expiry.
Using the <a href="https://cert-manager.io/docs/installation/">installation instructions</a> present on their website, I was able to set up <em>cert-manager</em> and the associated staging/production Letâ€™s Encrypt <em>ClusterIssuer</em> resources.
I was then able to add the required annotations to the Ingress resource, instructing <em>cert-manager</em> to issue and use a Letâ€™s Encrypt certificate.
I initially used the staging Letâ€™s Encrypt issuer to ensure that my configuration was correct before moving on to generating the production-ready certificate.
With the certificate in place, I could now access the web application on the public internet via HTTPS.</p>

<h2 id="conclusion">Conclusion</h2>

<p>By the end of the long weekend, the web application was finally live! ðŸŽ‰</p>

<p>I am very happy with the resulting Kubernetes-based deployment.
Getting the chance to construct and configure the Kubernetes cluster itself was a lot of fun and was made easier thanks to K3s and the k3sup tool.
Being able to keep so much of the original application intact and only apply additive changes was a great experience.
It highlighted the importance of the twelve-factor app methodology and the use of environment variables.
Getting hands-on experience using Kubernetes resources and tools such as Helm and cert-manager was invaluable.
Finally, being able to produce a deliverable on par with what is currently running in the AWS Lambda environment was very rewarding.</p>

<p>I hope this has piqued your interest in trying to set up and configure your own Kubernetes cluster!</p>

</article>

            </main>

            <footer class="footer">
                <img src="https://www.gravatar.com/avatar/c5c2978bb14d16460f73399c394b6acd?s=160">
                <ul>
                    <li>Developer at <a href="http://www.mybuilder.com/">MyBuilder</a></li>
                    <li><a href="http://threedevsandamaybe.com/">Three Devs and a Maybe</a> podcast co-host</li>
                    <li>All ramblings can be found in the <a href="/archive/">Archive</a></li>
                </ul>
                <div class="cf"></div>
            </footer>
        </div>
    </body>
</html>
