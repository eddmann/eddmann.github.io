<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Weeknotes: Throwaway Tools, Sessions as Trees, and Building a Personal AI Assistant - Edd Mann</title>
<meta name=description content="Debugging with throwaway CLIs and dashboards, shipping F1 Picks 2026 with pragmatic architecture and Chicago-school testing, rebranding MyPodcast with parallel work trees, representing agent sessions as tree structures, and building Jeeves - a personal AI assistant on a Raspberry Pi."><meta name=author content="Edd Mann"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZPQ7WHNXH4"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZPQ7WHNXH4")}</script><meta itemprop=name content="Weeknotes: Throwaway Tools, Sessions as Trees, and Building a Personal AI Assistant"><meta itemprop=description content="Week three. Itâ€™s been another busy week with work and spending any free time I do have with these LLMs, exploring many different things along the way."><meta itemprop=datePublished content="2026-02-08T00:00:00+00:00"><meta itemprop=dateModified content="2026-02-08T00:00:00+00:00"><meta itemprop=wordCount content="2742"><meta itemprop=keywords content="Weeknotes,Agents,Personal-Software,Testing,Llm"><meta property="og:url" content="https://eddmann.com/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/"><meta property="og:site_name" content="Edd Mann"><meta property="og:title" content="Weeknotes: Throwaway Tools, Sessions as Trees, and Building a Personal AI Assistant"><meta property="og:description" content="Week three. Itâ€™s been another busy week with work and spending any free time I do have with these LLMs, exploring many different things along the way."><meta property="og:locale" content="en_GB"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-08T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-08T00:00:00+00:00"><meta property="article:tag" content="Weeknotes"><meta property="article:tag" content="Agents"><meta property="article:tag" content="Personal-Software"><meta property="article:tag" content="Testing"><meta property="article:tag" content="Llm"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weeknotes: Throwaway Tools, Sessions as Trees, and Building a Personal AI Assistant"><meta name=twitter:description content="Week three. Itâ€™s been another busy week with work and spending any free time I do have with these LLMs, exploring many different things along the way."><meta name=twitter:site content="@edd_mann"><link rel=stylesheet href=/css/style.min.4c8b66c005db4efc68625e4ceed190f2dc110d342d05ffb19ba924cc13c3ef15.css integrity="sha256-TItmwAXbTvxoYl5M7tGQ8twRDTQtBf+xm6kkzBPD7xU="><link rel=preload as=image href=/assets/x.svg crossorigin=anonymous><link rel=preload as=image href=/assets/github.svg crossorigin=anonymous><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://eddmann.com/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/><script>document.documentElement.setAttribute("data-theme",localStorage.getItem("theme")||window.matchMedia("(prefers-color-scheme: dark)").matches&&"dark"||"light")</script><script src=/script.min.da71ac9c7b8bd4e644618df0ab735aed1393aad3cb5e5c57e5adc300fe7c8209.js integrity="sha256-2nGsnHuL1OZEYY3wq3Na7ROTqtPLXlxX5a3DAP58ggk=" defer></script></head><body><header class="site-header l-wrapper"><a href=https://eddmann.com/><h3 class=site-header__title>Edd Mann
<span style=--url:url(/assets/code.svg)>Developer</span></h3></a><button class=site-header__mobile-navigation-button type=button title="Toggle mobile site navigation" aria-label="Toggle mobile site navigation" aria-expanded=false aria-controls=site-navigation></button><div class=site-header__navigation id=site-navigation><nav aria-label="Primary navigation"><ul class=site-header__primary-navigation><li><a href=/archive>Archive</a></li><li><a href=/projects>Projects</a></li><li><a href=/about>About</a></li><li><a href=/cv.html>CV</a></li></ul></nav><nav aria-label="Social links"><ul class=site-header__social-navigation><li><a class=social-icon style=--url:url(/assets/x.svg) href=https://x.com/edd_mann rel="external noopener" target=_blank>Twitter (X)</a></li><li><a class=social-icon style=--url:url(/assets/github.svg) href=https://github.com/eddmann rel="external noopener" target=_blank>GitHub</a></li></ul></nav></div></header><main class=l-wrapper><article><header class=l-page-title><h1 class=u-transition-between-pages style=--id:weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant>Weeknotes: Throwaway Tools, Sessions as Trees, and Building a Personal AI Assistant</h1><time datetime=2026-02-08T00:00:00Z class=published-at>Feb 8, 2026</time></header><main class=u-prose><p>Week three.
It&rsquo;s been another busy week with work and spending any free time I do have with these LLMs, exploring many different things along the way.</p><h2 id=throwaway-tools-and-closing-the-loop>Throwaway Tools and Closing the Loop</h2><p>Something that&rsquo;s shifted fundamentally in how I debug: I build tools now.</p><p>Not permanent tools.
Throwaway CLIs, temporary dashboards, one-off commands - things that exist only to help me (or the agent) understand what&rsquo;s going on.
The cost of building these has dropped so dramatically that it&rsquo;s now cheaper to spin up a diagnostic tool than to manually inspect something.</p><p>This connects to what Peter Steinberger was talking about in the <a href="https://www.youtube.com/watch?v=8lF7HmQ_RgY" rel="external noopener" target=_blank>Pragmatic Engineer interview</a>: <strong>closing the loop</strong>.
How do you give an agent the ability to verify something that you&rsquo;d normally check yourself?
Build it a tool.
A CLI that queries the state, a dashboard that visualises the data, a command that exercises the edge case.</p><p>There are two levels here.
Some tools stick around - a CLI for inspecting database state, a health check endpoint.
Those get committed.
But many are genuinely disposable: a quick script to verify a hypothesis about why a calculation is off, a dashboard to visualise data flow while tracking down a subtle bug.
You build it, use it, delete it.</p><p>The key insight is that these tools serve both you <em>and</em> the agent.
You can see what&rsquo;s happening.
But more importantly, you can give the agent the ability to close its own feedback loop - to verify its work without asking you to check a screen or run a manual test.</p><p>This doesn&rsquo;t replace good test coverage - automated testing and regression suites are still essential.
But these temporary tools fill the role that writing tests for discovery used to.
Where in the past you&rsquo;d write a quick test to explore a hypothesis, now you can build a rich CLI or dashboard instead.</p><h2 id=f1-picks-2026>F1 Picks 2026</h2><p>With the F1 season approaching - pre-season testing in Bahrain is just around the corner - I needed to get the <a href=https://github.com/eddmann/f1-picks-2026 rel="external noopener" target=_blank>F1 Picks</a> website sorted.</p><p>For context: this is a <em>friendly</em> prediction game I&rsquo;ve been running with my family for a couple of years.
You pick drivers, there&rsquo;s a point system, there&rsquo;s a winner at the end.</p><p>What&rsquo;s interesting is the progression.
When I built the <a href=https://github.com/eddmann/f1-picks rel="external noopener" target=_blank>2024 version</a>, it took about four hours.
At the time, I thought that was impressive - flinging code between ChatGPT and my IDE, lots of manual copy-paste between the model and the editor.
Very much a person in the middle.</p><p>The 2026 version? About an hour.
Full new system on Cloudflare with a D1 database, a completely reworked point system that I explored and validated with the family, comprehensive test coverage, solid architecture.
And this isn&rsquo;t vibe coding - I ran it through the same deliberate process I described <a href=../2026-01-31-weeknotes-model-personalities-building-my-own-agent-and-two-schools-of-agentic-development/>last week</a>: explore with Opus 4.5, solidify with Codex 5.2, then implement with patterns established.</p><p><picture><source type=image/webp srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-dashboard_hu_10de1b0bf71b5e2d.webp 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-dashboard_hu_f27316317e0da95f.webp 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-dashboard_hu_a6a7888571fe38ad.webp 744w"><source type=image/jpeg srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-dashboard_hu_b08c78c4d3e620f8.jpg 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-dashboard_hu_c806eab2195332f2.jpg 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-dashboard_hu_e2705d9b8074d705.jpg 744w"><img src=/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-dashboard_hu_c806eab2195332f2.jpg alt="F1 Picks 2026 - Dashboard" loading=lazy>
</picture><picture><source type=image/webp srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-calendar_hu_59b12b09d5be3930.webp 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-calendar_hu_cf126a31e825edb7.webp 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-calendar_hu_83c1df022e9199d0.webp 744w"><source type=image/jpeg srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-calendar_hu_c015f2f838c3542c.jpg 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-calendar_hu_ad2f971190d55b72.jpg 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-calendar_hu_3d205ab0fd0cff08.jpg 744w"><img src=/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/f1-picks-calendar_hu_ad2f971190d55b72.jpg alt="F1 Picks 2026 - Calendar" loading=lazy></picture></p><p>My expectations of what I can ship in a short space of time have fundamentally changed.
Four hours felt fast two years ago.
Now an hour gets me something with more polish, more features, better testing, and better architecture.
I also finally released <a href=https://github.com/eddmann/step-wars rel="external noopener" target=_blank>Step Wars</a> this week - the Star Wars-themed step competition I mentioned previously.</p><h2 id=the-pragmatic-middle-architecture-and-testing>The Pragmatic Middle: Architecture and Testing</h2><p>Both F1 Picks and Step Wars went through the same architectural decision process, and it&rsquo;s worth talking about because the approach lands in an interesting spot.</p><p>The agent and I discussed this extensively - hexagonal architecture, clean architecture, onion architecture.
Full domain models with symmetric ports (both driving and driven).
We decided against all of that.</p><p>Instead, we went <strong>pragmatic</strong>.
The core principle from all these approaches is the same: depend on abstractions, not implementations.
So the driven ports are there - all infrastructure concerns (database, external APIs) are abstracted behind interfaces that can be tested in isolation.
But there are no driving port adapters.
The delivery layer calls the use cases (really just <em>fat</em> application services) directly, and there&rsquo;s no distinct domain model layer underneath - shared types are used as-is.</p><p>The testing strategy matters just as much.
I&rsquo;m a big fan of the Chicago school - test behaviour at the public API boundary, not implementation details.
This is what <a href="https://www.youtube.com/watch?v=EZ05e7EMOLM" rel="external noopener" target=_blank>Ian Cooper talks about</a> in his &ldquo;TDD, Where Did It All Go Wrong&rdquo; talk: test the behaviour from the outside, <em>double</em> the collaborators, and you get tests that tell you <em>what</em> broke, not just <em>that</em> something broke.</p><p>The reason you test at different levels is about <strong>localised, timely feedback</strong>.
A broad end-to-end test tells you something is broken, but the problem could be anywhere in the stack - you&rsquo;ve gained confidence but not locality.
A focused unit (of behaviour) test around a use case with external concerns <em>doubled</em> out narrows the blast radius: when it fails, you know <em>where</em> the problem is, and you know <em>fast</em>.
The more localised the test, the tighter the feedback loop.</p><p>What&rsquo;s rewarding is being able to discuss this <em>with</em> the agent at a high level - &ldquo;what architecture pattern fits here?&rdquo;, &ldquo;how should we approach testing?&rdquo;, &ldquo;should we use MSW for the client?&rdquo; - and then have it implement those decisions consistently across both the client and the API.
Write it up in the documentation, get alignment, and the agent follows the patterns.
These models follow guidelines much more reliably now than even a few months ago.</p><h2 id=demo-data-and-the-screenshots-problem>Demo Data and the Screenshots Problem</h2><p>Working at this velocity creates a new problem: screenshots lag behind the code.</p><p>The friction of getting an application into a specific visual state - for README images, landing pages, App Store screenshots - adds up.
I noticed this particularly with <a href=https://github.com/eddmann/VoiceScribe rel="external noopener" target=_blank>VoiceScribe</a>, where the screenshots had fallen embarrassingly behind the actual app ðŸ˜¬.</p><p>So I built <strong>demo states</strong> into my applications.
Every app now has the concept of demo data - a way to put the application into specific, useful states for screenshots, verification, or visual testing.</p><p>The challenge is balance.
Like logging, you don&rsquo;t want demo data concerns polluting your production code.
And there are performance considerations.
I spent time thinking about this in both JavaScript/React contexts (<a href=https://github.com/eddmann/revu rel="external noopener" target=_blank>revu</a>, the review app) and iOS/Swift contexts (Ovlo, ClaudeMeter, VoiceScribe, ClipVault).
Using tree-shaking in the JavaScript context and <code>#if DEBUG</code> macros in Swift to ensure this logic isn&rsquo;t included in release builds.</p><p>For iOS, I explored using SwiftUI previews since they&rsquo;re built for this, but they don&rsquo;t give the exact same feel as a simulator.
So the demo states boot the actual application into a specific configuration that can be screenshotted from the simulator.</p><p><picture><source type=image/webp srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/revu-demo_hu_962f6db40eb6fc9.webp 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/revu-demo_hu_95f66b28a3c4c8.webp 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/revu-demo_hu_2febc27cc7b98a08.webp 1400w"><source type=image/jpeg srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/revu-demo_hu_e05f69dd58b11b49.jpg 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/revu-demo_hu_2a71bfde7d2a757.jpg 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/revu-demo_hu_ab97c7155a1199f0.jpg 1400w"><img src=/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/revu-demo_hu_2a71bfde7d2a757.jpg alt="revu with demo data" loading=lazy></picture></p><p>One detail I find satisfying: <strong>aesthetically pleasing data</strong>.
It&rsquo;s easy to be lazy about what example names look like, how many items to show, how much whitespace there is.
But these screenshots represent your project.
The data should feel real, use the space well, and showcase features.
It&rsquo;s like the old observation about LLMs always generating <a href=https://medium.com/@SamMormando/why-ai-thinks-every-clock-says-10-10-a-simple-way-to-talk-about-bias-in-the-classroom-97b9122e7a77 rel="external noopener" target=_blank>clocks showing 10:10</a> - that&rsquo;s the most aesthetically balanced position.
Same principle.</p><h2 id=mypodcast-rebrand>MyPodcast Rebrand</h2><p>I picked <a href=https://mypodcast.cloud/ rel="external noopener" target=_blank>MyPodcast</a> back up this week.
It turns articles, blog posts, and YouTube videos you save into a personal podcast feed - hear what you&rsquo;d otherwise never get round to reading.</p><p>The UI had been bothering me.
The mascot idea felt <em>dated</em>, the design wasn&rsquo;t clean enough, and it didn&rsquo;t work well across screen sizes.</p><p>So I had the agent spin up <strong>5 git worktrees</strong>, each exploring a radically different design direction using the frontend-design skill with Opus 4.5.
Each one got its own HTTP server so I could preview them side by side.</p><p>The process:</p><ol><li>Generate five diverse designs in parallel worktrees</li><li>Preview them all via HTTP servers at different ports</li><li>Pick the best direction, pull ideas from others</li><li>Collapse back to the main branch with the chosen design</li><li>Iterate from there</li></ol><p>The migration to Tailwind was a clear win - these models are excellent with Tailwind because the training data is so rich.
The new design is more mobile-responsive, cleaner, and more professional.
I then applied the same styling to the <a href=https://mypodcast.cloud/browser-extension rel="external noopener" target=_blank>browser extension</a> too, so the whole experience feels cohesive.</p><p><picture><source type=image/webp srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-rebrand_hu_c66c1610a20df180.webp 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-rebrand_hu_87fac5323995a67e.webp 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-rebrand_hu_7b3f3870e0c3edaa.webp 1400w"><source type=image/jpeg srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-rebrand_hu_c85c64b78acac43e.jpg 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-rebrand_hu_31b295a6e48eeb83.jpg 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-rebrand_hu_5430ce5b95ffbc56.jpg 1400w"><img src=/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-rebrand_hu_31b295a6e48eeb83.jpg alt="MyPodcast rebrand" loading=lazy></picture></p><p>It&rsquo;s not where I want it to be yet, but it&rsquo;s a better direction.
Forward progress.</p><h2 id=mypodcast-goes-mobile>MyPodcast Goes Mobile</h2><p>The browser extension for MyPodcast works well for capturing content.
When you save an article, it dumps the full rendered HTML - complete content regardless of SPAs, paywalls, or dynamic rendering.
No proxying or server-side rendering needed.</p><p>But I find most articles I want to save when I&rsquo;m on my phone.
And the web-based save wasn&rsquo;t cutting it.</p><p>So I built a React Native app.
I chose React Native specifically because LLMs are exceptionally good with JavaScript, TypeScript, and React - the training data is dense enough that it just flows.</p><p>The key feature is a <strong>share extension</strong>.
From any mobile browser, you share a link with MyPodcast.
It opens in a WebView within the app, previews the article, and when you confirm, saves the full rendered HTML content - the same approach as the browser extension.
Same API endpoint, same backend processing.</p><p><picture><source type=image/webp srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-mobile-app_hu_dcb6c20a8de3d640.webp 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-mobile-app_hu_b7daa15eaae198f8.webp 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-mobile-app_hu_26f00064a3df55d0.webp 1179w"><source type=image/jpeg srcset="/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-mobile-app_hu_b431d3fec278c222.jpg 350w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-mobile-app_hu_b7d4f341773174a.jpg 700w, /posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-mobile-app_hu_9a1de910b30a0fbc.jpg 1179w"><img src=/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/mypodcast-mobile-app_hu_b7d4f341773174a.jpg alt="MyPodcast mobile app" loading=lazy></picture></p><p>This solves the content capture problem elegantly: the user&rsquo;s browser (or WebView) does the rendering, and I get the complete content regardless of how the page was built.
No need for server-side Puppeteer or Playwright.
The content the person saw is exactly what gets saved.</p><p>I&rsquo;m testing it through an Expo development build and plan to release it next week.
Works on both iOS and Android, styled in the new rebrand.</p><h2 id=coding-agent-sessions-as-trees>Coding Agent: Sessions as Trees</h2><p>I continued work on <a href=https://github.com/eddmann/my-own-coding-agent rel="external noopener" target=_blank>My Own Coding Agent</a> this week, with a big focus on session management.</p><p>The initial approach was crude - destructive compaction, simple message lists.
What I landed on, inspired by how <a href=https://github.com/badlogic/pi-mono rel="external noopener" target=_blank>Pi</a> handles it, is an <strong>append-only JSONL format</strong> where sessions are represented as a <strong>tree data structure</strong>.</p><p>This means conversations are non-destructive.
You can start a conversation, go back to an earlier point, branch off in a new direction, come back to the original branch and continue.
The implementation is clean: you just need a pointer to the current node, and you can reconstruct any conversation path by traversing up to the root.
<a href=https://mariozechner.at/ rel="external noopener" target=_blank>Mario</a> had been talking about sessions as tree structures, and it really is beautiful once you see it.</p><p>There&rsquo;s also a <strong>fork</strong> capability: take one path from root to the current node and create a completely new session file from it.
Fresh start, but with all the prior context preserved.</p><p>Compaction is non-destructive now too.
It appends a compacted summary rather than destroying history.
Everything append-only, which makes the whole system much more robust.</p><p><strong>Model switching</strong> across sessions surfaced interesting nuances.
Each model carries metadata - response IDs, caching hints, implementation-specific details.
When switching from GPT-5.2-Codex to Opus 4.5 mid-session, I need to carry the right metadata for each model but strip what&rsquo;s irrelevant for the next.
The append-only format handles this by decorating messages with which model produced them.</p><p>I also added <strong>Max/Pro Plan authentication</strong> for Anthropic.
Interesting timing - I experienced firsthand the drama that hit <a href=https://github.com/opencode-ai/opencode rel="external noopener" target=_blank>OpenCode</a>, Pi, and other agent harnesses when Anthropic started blocking API requests using this authentication method.
The spoofing is whack-a-mole (renaming tools to look like Claude Code, for example), but keeping it at the API provider abstraction level means the rest of the codebase doesn&rsquo;t care about these details.</p><p>By the end of the week I had <strong><a href=https://www.anthropic.com/news/claude-opus-4-6 rel="external noopener" target=_blank>Opus 4.6</a></strong> support working - its adaptive thinking mode requiring a unified abstraction at the LLM provider level - and I&rsquo;m working on <strong><a href=https://openai.com/index/introducing-gpt-5-3-codex/ rel="external noopener" target=_blank>GPT-5.3-Codex</a></strong> support too.
Switching between Codex and Opus mid-session was a satisfying moment.</p><p>I also put together a demo of the agent building a simple LISP interpreter from scratch - reading the problem, implementing it in TypeScript with Bun, testing it, and presenting the results.</p><p><img src=/posts/weeknotes-throwaway-tools-sessions-as-trees-and-building-a-personal-ai-assistant/agent-lisp-demo.gif alt="My Own Coding Agent building a LISP interpreter" loading=lazy></p><p>Seeing your own agent harness work through a problem end-to-end is deeply satisfying.</p><h2 id=jeeves-a-personal-ai-assistant>Jeeves: A Personal AI Assistant</h2><p>The conversation that really set my mind going this week was with <a href=https://tjmiller.me/ rel="external noopener" target=_blank>TJ Miller</a>.
I&rsquo;ll be releasing that podcast episode on <a href=https://compiledconversations.com/ rel="external noopener" target=_blank>Compiled Conversations</a> in the coming weeks.
He&rsquo;s been building <a href=https://iris.prismphp.com/ rel="external noopener" target=_blank>Iris</a> - a personal AI assistant with a sophisticated memory system - and his approach to memory is genuinely fascinating.</p><p>His work on <a href=https://www.tjmiller.me/posts/iris-truths/ rel="external noopener" target=_blank>Truths</a> - separating identity-defining facts from episodic memories - and <a href=https://www.tjmiller.me/posts/multi-generational-memory-consolidation/ rel="external noopener" target=_blank>multi-generational memory consolidation</a> gave me a lot to think about.
The <a href=https://www.tjmiller.me/posts/truths-refinement-update/ rel="external noopener" target=_blank>refinement update</a> on contradiction detection and generation tracking is especially interesting.</p><p>That conversation, combined with listening to <a href=https://syntax.fm/show/976/pi-the-ai-harness-that-powers-openclaw-w-armin-ronacher-and-mario-zechner rel="external noopener" target=_blank>Mario and Armin on Syntax.fm</a> discussing Pi and <a href=https://openclaw.ai/ rel="external noopener" target=_blank>OpenClaw</a>, pushed me to build my own.</p><p>I&rsquo;d been looking more closely at OpenClaw, and then I noticed <a href=https://github.com/gavrielc/nanoclaw rel="external noopener" target=_blank>NanoClaw</a> - a stripped-down, easy to understand alternative.
NanoClaw&rsquo;s philosophy resonated: small enough to understand, secure by container isolation, built for one user.</p><p>So I built <strong><a href=https://github.com/eddmann/jeeves rel="external noopener" target=_blank>Jeeves</a></strong> - my personal AI assistant, housed in a Docker container running on a Raspberry Pi.
It sits somewhere between NanoClaw&rsquo;s simplicity and OpenClaw&rsquo;s ambition.</p><p>Key design decisions:</p><ul><li><strong>Telegram only</strong> - it&rsquo;s what I use</li><li><strong>Anthropic only</strong> - one provider, done well</li><li><strong>Own agent harness</strong> - not NanoClaw&rsquo;s SDK-based approach, so I control sessions, memory, and the full agent lifecycle</li><li><strong>Code-first skills</strong> - UV inline scripts, Bun scripts, phpx scripts. The skills <em>are</em> code. No MCP, no tool-calling wrappers. The LLM writes code to solve problems.</li></ul><p>The real unlock is <strong>proactive skill creation</strong>.
When Jeeves encounters a problem or discovers a capability I want, it builds its own skill - and uses it going forward.
And it can <em>update</em> those skills after the fact.</p><p>I had a great example with Plex integration.
Jeeves built a UV inline script for interacting with my Plex server - authentication, library queries, playback history.
During the initial build, authentication wasn&rsquo;t working.
I could see Jeeves reasoning about the problem, going back to the skill, identifying the auth issue, updating the code, and retrying.
Self-correcting skill development.</p><p>This pattern of using libraries directly - the Plex Python library, <a href=https://playwright.dev/ rel="external noopener" target=_blank>Playwright</a> for browser automation, <a href=https://github.com/octokit/octokit.js rel="external noopener" target=_blank>Octokit</a> for GitHub - feels more powerful than wrapping everything in CLIs.</p><p>For memory, the current approach is straightforward: the context window is the last 50 messages, and a set of core memory files (like <code>MEMORY.md</code> and <code>USER.md</code>) are always injected into the system prompt.
There are also daily memory files (<code>memory/YYYY-MM-DD.md</code>) for episodic notes.
The assistant is given guidance to update both core and daily memory when it sees fit - there&rsquo;s no forced sync point or trigger, it just maintains its own memory as the conversation flows.
Over the next few weeks I want to explore memory more deeply - TJ&rsquo;s work on Truths and multi-generational consolidation, along with the OpenClaw and NanoClaw approaches, have given me a lot of ideas.</p><h2 id=what-ive-been-watchinglistening-to>What I&rsquo;ve Been Watching/Listening To</h2><p><strong>Articles:</strong></p><ul><li><a href=https://www.tjmiller.me/posts/iris-truths/ rel="external noopener" target=_blank>Introducing Truths</a> - TJ Miller on separating universal facts from contextual memories</li><li><a href=https://www.tjmiller.me/posts/truths-refinement-update/ rel="external noopener" target=_blank>Truths: The Refinement Update</a> - contradiction detection and generation tracking in Iris</li><li><a href=https://www.tjmiller.me/posts/multi-generational-memory-consolidation/ rel="external noopener" target=_blank>Multi-Generational Memory Consolidation</a> - how Iris consolidates memories over time</li><li><a href=https://www.tjmiller.me/posts/building-nova-foundation/ rel="external noopener" target=_blank>Building Nova - The Foundation</a> - the origins of TJ&rsquo;s AI companion project</li><li><a href=https://registerspill.thorstenball.com/p/joy-and-curiosity-72 rel="external noopener" target=_blank>Joy & Curiosity #72</a> - Thorsten Ball on why some developers still don&rsquo;t see what AI can do</li><li><a href=https://registerspill.thorstenball.com/p/joy-and-curiosity-73 rel="external noopener" target=_blank>Joy & Curiosity #73</a> - optimising code for agents rather than humans</li></ul><p><strong>Podcasts:</strong></p><ul><li><a href="https://www.youtube.com/watch?v=4rx36wc9ugw" rel="external noopener" target=_blank>Raising An Agent #10</a> - Amp killing their VS Code sidebar extension, &ldquo;deep mode&rdquo; with GPT-5.2-Codex as a different way of working, and why different models need different modes not just drop-down selections</li><li><a href=https://lexfridman.com/ai-sota-2026 rel="external noopener" target=_blank>State of AI in 2026</a> - Lex Fridman with Nathan Lambert and Sebastian Raschka on the state of play - no secret tech advantages, Anthropic&rsquo;s bet on code paying off, and DeepSeek sparking a whole Chinese open-weight ecosystem</li><li><a href=https://www.dwarkesh.com/p/elon-musk rel="external noopener" target=_blank>Elon Musk on Dwarkesh Podcast</a> - AI in space in 36 months, consciousness as a tiny candle in vast darkness, and robotics as the only path to genuine abundance</li><li><a href=https://syntax.fm/show/976/pi-the-ai-harness-that-powers-openclaw-w-armin-ronacher-and-mario-zechner rel="external noopener" target=_blank>Pi: The AI Harness That Powers OpenClaw</a> - Armin Ronacher and Mario Zechner on Syntax.fm discussing agent design, memory, and why &ldquo;Bash is all you need&rdquo;</li></ul><hr><p>The theme that keeps emerging: the tools are good enough now that the bottleneck is deciding <em>what</em> to build, not <em>whether</em> you can build it.
Architecture, testing, and thoughtful design matter more than ever - not less - because agents amplify whatever foundation you give them.</p><p>And of course by the end of the week Anthropic and OpenAI dropped their latest models, <a href=https://www.anthropic.com/news/claude-opus-4-6 rel="external noopener" target=_blank>Opus 4.6</a> and <a href=https://openai.com/index/introducing-gpt-5-3-codex/ rel="external noopener" target=_blank>GPT-5.3-Codex</a>.
I look forward to getting a feel for their character over the coming weeks.</p></main><footer class=post-footer><ul class="tags tags--large"><li><a href=/archive/tag/weeknotes>weeknotes</a></li><li><a href=/archive/tag/agents>agents</a></li><li><a href=/archive/tag/personal-software>personal-software</a></li><li><a href=/archive/tag/testing>testing</a></li><li><a href=/archive/tag/llm>llm</a></li></ul><div class=related><h3 class=related__title>Related Posts</h3><ul class=related__posts><li><a href=/posts/weeknotes-model-personalities-building-my-own-agent-and-two-schools-of-agentic-development/>Weeknotes: Model Personalities, Building My Own Agent, and Two Schools of Agentic Development</a></li><li><a href=/posts/weeknotes-from-mcp-to-skills-buns-phpx-and-personal-software/>Weeknotes: From MCP to Skills, buns, phpx, and the Rise of Personal Software</a></li><li><a href=/posts/running-mcps-everywhere-chatting-with-my-workouts/>Running MCPs Everywhere: Chatting with My Workouts</a></li><li><a href=/posts/contextual-movie-conversations-building-a-plex-mcp-server/>Contextual Movie Conversations: Building a Plex MCP Server</a></li><li><a href=/posts/santa-lang-workshop-exploring-agentic-llm-workflows-for-language-implementation/>santa-lang Workshop: Exploring Agentic LLM Workflows for Language Implementation</a></li></ul></div></footer></article><div class="podcast-ad u-overlay-wrapper"><div class=podcast-ad__artwork><img src=https://compiledconversations.com/album-art.jpg alt="Compiled Conversations podcast album art" class=podcast-ad__artwork-image></div><div class=podcast-ad__content><h3 class=podcast-ad__title>Compiled Conversations</h3><p class=podcast-ad__description>Podcast I host, featuring conversations with the people shaping software and technology.</p><div class=podcast-ad__link>Check out the show</a></div><a class=u-overlay href=https://compiledconversations.com target=_blank>Listen to Compiled Conversations</a></div><div class=scroll-watcher></div></main><footer class="site-footer l-wrapper"><div>&copy; 2026, Edd Mann</div><button class=site-footer__theme-toggle type=button title="Toggle theme" aria-label="Toggle theme"><svg fill="currentcolor" viewBox="0 0 32 32" role="img"><title>Theme toggle icon</title><desc>A circle representing the moon for toggling theme</desc><path d="M16 .5C7.4.5.5 7.4.5 16S7.4 31.5 16 31.5 31.5 24.6 31.5 16 24.6.5 16 .5zm0 28.1V3.4C23 3.4 28.6 9 28.6 16S23 28.6 16 28.6z"/></svg></button></footer></body></html>