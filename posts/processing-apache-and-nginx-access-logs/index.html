<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Processing Apache and Nginx Access Logs - Edd Mann</title>
<meta name=description content="Discover efficient methods to process Apache and Nginx access logs using simple Unix commands. Extract valuable insights such as 404 errors, user agents, IP addresses and more to optimise your server analysis."><meta name=author content="Edd Mann"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZPQ7WHNXH4"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZPQ7WHNXH4")}</script><meta itemprop=name content="Processing Apache and Nginx Access Logs"><meta itemprop=description content="Tools such as AWStats and Logstalgia are great, but sometimes they can be overkill for the problem we are trying to solve. It turns out that with a couple of simple Unix commands we can gather a lot of useful information from the data stored in the access logs. Both Apache and Nginx by default use the combined log format, which I will be basing this post’s examples on. Below are two different methods of accessing either an uncompressed single file or multiple compressed files (following the supplied wild-card pattern)."><meta itemprop=datePublished content="2013-12-21T00:00:00+00:00"><meta itemprop=dateModified content="2013-12-21T00:00:00+00:00"><meta itemprop=wordCount content="662"><meta itemprop=keywords content="Apache,Nginx"><meta property="og:url" content="https://eddmann.com/posts/processing-apache-and-nginx-access-logs/"><meta property="og:site_name" content="Edd Mann"><meta property="og:title" content="Processing Apache and Nginx Access Logs"><meta property="og:description" content="Tools such as AWStats and Logstalgia are great, but sometimes they can be overkill for the problem we are trying to solve. It turns out that with a couple of simple Unix commands we can gather a lot of useful information from the data stored in the access logs. Both Apache and Nginx by default use the combined log format, which I will be basing this post’s examples on. Below are two different methods of accessing either an uncompressed single file or multiple compressed files (following the supplied wild-card pattern)."><meta property="og:locale" content="en_GB"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2013-12-21T00:00:00+00:00"><meta property="article:modified_time" content="2013-12-21T00:00:00+00:00"><meta property="article:tag" content="Apache"><meta property="article:tag" content="Nginx"><meta name=twitter:card content="summary"><meta name=twitter:title content="Processing Apache and Nginx Access Logs"><meta name=twitter:description content="Tools such as AWStats and Logstalgia are great, but sometimes they can be overkill for the problem we are trying to solve. It turns out that with a couple of simple Unix commands we can gather a lot of useful information from the data stored in the access logs. Both Apache and Nginx by default use the combined log format, which I will be basing this post’s examples on. Below are two different methods of accessing either an uncompressed single file or multiple compressed files (following the supplied wild-card pattern)."><meta name=twitter:site content="@edd_mann"><link rel="preload stylesheet" as=style href=/css/style.min.38ebe85b7a95b0ed5f3ec14e02666c3ac5666f40e38576814a1bdbdf87e4f477.css integrity="sha256-OOvoW3qVsO1fPsFOAmZsOsVmb0DjhXaBShvb34fk9Hc="><link rel=preload as=image href=/assets/x.svg><link rel=preload as=image href=/assets/github.svg><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://eddmann.com/posts/processing-apache-and-nginx-access-logs/><script>(function(){document.documentElement.setAttribute("data-theme",localStorage.getItem("theme")||window.matchMedia("(prefers-color-scheme: dark)").matches&&"dark"||"light")})()</script></head><body><header class="site-header wrapper"><a href=https://eddmann.com/><h3 class=site-header__title>Edd Mann
<span style=--url:url(/assets/code.svg)>Developer</span></h3></a><button class=site-header__mobile-navigation-button type=button title="Toggle mobile site navigation" aria-label="Toggle mobile site navigation" aria-expanded=false aria-controls=site-navigation></button><div class=site-header__navigation id=site-navigation><nav aria-label="Primary navigation"><ul class=site-header__primary-navigation><li><a href=/archive>Archive</a></li><li><a href=/projects>Projects</a></li><li><a href=/about>About</a></li></ul></nav><nav aria-label="Social links"><ul class=site-header__social-navigation><li><a class=social-icon style=--url:url(/assets/x.svg) href=https://x.com/edd_mann rel="external noopener" target=_blank>Twitter (X)</a></li><li><a class=social-icon style=--url:url(/assets/github.svg) href=https://github.com/eddmann rel="external noopener" target=_blank>GitHub</a></li></ul></nav></div></header><main class=wrapper><article><header class=page-title><h1 class=transition-between-pages style=--id:processing-apache-and-nginx-access-logs>Processing Apache and Nginx Access Logs</h1><time class=post__time>Dec 21, 2013</time></header><main class=prose><p>Tools such as <a href=http://awstats.sourceforge.net/ rel="external noopener" target=_blank>AWStats</a> and <a href=http://code.google.com/p/logstalgia/ rel="external noopener" target=_blank>Logstalgia</a> are great, but sometimes they can be overkill for the problem we are trying to solve.
It turns out that with a couple of simple Unix commands we can gather a lot of useful information from the data stored in the access logs.
Both Apache and Nginx by default use the <a href=http://httpd.apache.org/docs/1.3/logs.html#combined rel="external noopener" target=_blank>combined</a> log format, which I will be basing this post&rsquo;s examples on.
Below are two different methods of accessing either an uncompressed single file or multiple compressed files (following the supplied wild-card pattern).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ cat access.log <span style=color:#75715e># uncompressed file</span>
</span></span><span style=display:flex><span>$ zcat access.log-*.gz <span style=color:#75715e># compressed files i.e. access.log-20131221.gz</span>
</span></span></code></pre></div><h2 id=log-data-to-information>Log Data to Information</h2><h3 id=404-request-responses>404 Request Responses</h3><p>Using &lsquo;awk&rsquo; we are able to filter the access log entries down to only the ones that include the 404 status code.
In this case we are then displaying the most requested pages of this type, in descending order.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ cat access.log | awk <span style=color:#e6db74>&#39;($9 ~ /404/)&#39;</span> | awk <span style=color:#e6db74>&#39;{ print $7 }&#39;</span> | sort | uniq -c | sort -rn | head -n <span style=color:#ae81ff>25</span>
</span></span></code></pre></div><h3 id=request-user-agents>Request User Agents</h3><p>The command below displays the top 25 user agents (browser, operating system) that have requested a resource from the web server.
The &lsquo;awk&rsquo; command in this instance uses a defined field separator of " to successfully parse the user agent string.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ cat access.log | awk -F<span style=color:#ae81ff>\&#34;</span> <span style=color:#e6db74>&#39;{ print $6 }&#39;</span> | sort | uniq -c | sort -frn | head -n <span style=color:#ae81ff>25</span>
</span></span></code></pre></div><h3 id=request-ip-addresses>Request IP Addresses</h3><p>Below are two examples of displaying the top 25 IP addresses based on total requests.
The second command uses the GeoIP package to include the country the IP address originates from.
This package can be installed on a CentOS setup using the <a href=https://fedoraproject.org/wiki/EPEL rel="external noopener" target=_blank>EPEL</a> repository.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ cat access.log | awk <span style=color:#e6db74>&#39;{ print $1 }&#39;</span> | sort | uniq -c | sort -rn | head -n <span style=color:#ae81ff>25</span>
</span></span><span style=display:flex><span>$ cat access.log | awk <span style=color:#e6db74>&#39;{ print $1 }&#39;</span> | sort | uniq -c | sort -rn | head -n <span style=color:#ae81ff>25</span> | <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    awk <span style=color:#e6db74>&#39;{ printf(&#34;%5d\t%-15s\t&#34;, $1, $2); system(&#34;geoiplookup &#34; $2 &#34; | cut -d \\: -f2 &#34;) }&#39;</span>
</span></span></code></pre></div><h3 id=count-unique-visits>Count Unique Visits</h3><p>The commands below provide you with a total count of unique visits based on IP address.
You are also able to run the log file through the &lsquo;grep&rsquo; command before processing, to only include entries that have occurred today or this month.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ cat access.log | awk <span style=color:#e6db74>&#39;{ print $1 }&#39;</span> | sort | uniq -c | wc -l
</span></span><span style=display:flex><span><span style=color:#75715e># or today</span>
</span></span><span style=display:flex><span>$ cat access.log | grep <span style=color:#e6db74>`</span>date <span style=color:#e6db74>&#39;+%e/%b/%G&#39;</span><span style=color:#e6db74>`</span> | awk <span style=color:#e6db74>&#39;{ print $1 }&#39;</span> | sort | uniq -c | wc -l
</span></span><span style=display:flex><span><span style=color:#75715e># or this month</span>
</span></span><span style=display:flex><span>$ cat access.log | grep <span style=color:#e6db74>`</span>date <span style=color:#e6db74>&#39;+%b/%G&#39;</span><span style=color:#e6db74>`</span> | awk <span style=color:#e6db74>&#39;{ print $1 }&#39;</span> | sort | uniq -c | wc -l
</span></span></code></pre></div><h3 id=ranked-by-response-codes>Ranked by Response Codes</h3><p>This simple command is very useful to quickly observe the total counts based on returned response code.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ cat access.log | awk <span style=color:#e6db74>&#39;{ print $9 }&#39;</span> | sort | uniq -c | sort -rn
</span></span></code></pre></div><h3 id=most-popular-urls>Most Popular URLS</h3><p>A trivial replacement for some Google Analytics statistics, reporting how many hits the top 25 resources have tallied.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ cat access.log | awk <span style=color:#e6db74>&#39;{ print $7 }&#39;</span> | sort | uniq -c | sort -rn | head -n <span style=color:#ae81ff>25</span>
</span></span></code></pre></div><h3 id=real-time-ip-page-requests>Real-time IP-Page Requests</h3><p>The final two commands are most likely my favourite as they provide me with real-time access information.
These commands report on each IP address, request and response that have recently occurred on the server.
Using <a href=http://linuxcommand.org/man_pages/tailf1.html rel="external noopener" target=_blank>tailf</a> instead of a typical <code>tail -f</code> has the benefit of not accessing the file when it is not growing.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ tailf access.log | awk <span style=color:#e6db74>&#39;{ printf(&#34;%-15s\t%s\t%s\t%s\n&#34;, $1, $6, $9, $7) }&#39;</span>
</span></span><span style=display:flex><span>$ tailf access.log | awk <span style=color:#e6db74>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;geoiplookup &#34; $1 &#34; | cut -d \\: -f2 &#34; | getline geo
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    printf(&#34;%-15s\t%s\t%s\t%-20s\t%s\n&#34;, $1, $6, $9, geo, $7);
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  }&#39;</span>
</span></span></code></pre></div><h2 id=resources>Resources</h2><ul><li><a href=http://www.the-art-of-web.com/system/logs/ rel="external noopener" target=_blank>System: Analyzing Apache Log Files</a></li><li><a href=https://rtcamp.com/tutorials/nginx/log-parsing/ rel="external noopener" target=_blank>Parsing access.log and error.logs using linux commands</a></li><li><a href=http://stackoverflow.com/questions/16128472/how-to-gather-ip-and-user-agent-info-from-web-log-with-awk rel="external noopener" target=_blank>How to gather IP and User Agent info from web log with AWK?</a></li></ul></main><footer class=post__tags><a href=/archive/tag/apache>apache</a><a href=/archive/tag/nginx>nginx</a></footer></article><div class="related-posts prose"><h3>Related Posts</h3><ul><li><a href=/posts/installing-nginx-apache-mysql-php-5-5-lamp-stack-on-cent-os-6-4/>Installing Nginx/Apache, MySQL, PHP 5.5 (LAMP) stack on CentOS 6.4</a></li></ul></div><div class=scroll-watcher></div></main><footer class="site-footer wrapper"><div>&copy; 2025, Edd Mann</div><button class=theme-toggle type=button title="Toggle theme" aria-label="Toggle theme"><svg fill="currentcolor" viewBox="0 0 32 32" role="img"><title>Theme toggle icon</title><desc>A circle representing the moon for toggling theme</desc><path d="M16 .5C7.4.5.5 7.4.5 16S7.4 31.5 16 31.5 31.5 24.6 31.5 16 24.6.5 16 .5zm0 28.1V3.4C23 3.4 28.6 9 28.6 16S23 28.6 16 28.6z"/></svg></button></footer><script>(function(){document.querySelector(".theme-toggle").addEventListener("click",()=>{localStorage.setItem("theme",localStorage.getItem("theme")==="dark"?"light":"dark"),document.documentElement.setAttribute("data-theme",localStorage.getItem("theme"))}),document.querySelector(".site-header__mobile-navigation-button").addEventListener("click",e=>{document.documentElement.classList.toggle("mobile-navigation-open"),e.target.setAttribute("aria-expanded",e.target.getAttribute("aria-expanded")==="false"?"true":"false")});const n=document.querySelector(".site-header");let t=window.pageYOffset,e=!1;window.addEventListener("scroll",()=>{if(e)return;setTimeout(()=>{const s=window.pageYOffset;n.classList.toggle("is-sticky",s>0&&t>s),t=s,e=!1},500),e=!0})})()</script></body></html>